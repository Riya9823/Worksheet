1. C
2. D
3. C
4. A
5. A
6. C
7. C
8. B,C
9. B,C,D
10. A,D
11. An outlier is a piece of data that is an abnormal distance from other points. In other words, it’s data that lies outside the other values in the set. 
    The Inter Quartile Range(IQR) method for outlier detection are:
    1.Calculate the interquartile range for the data.
    2.Multiply the interquartile range (IQR) by 1.5.
    3.Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier.
    4.Subtract 1.5 x (IQR) from the first quartile. Any number less than this is a suspected outlier.
Example: {0, 1, 2, 4, 5, 5, 7, 10, 10, 12, 13, 17, 39}
         Q1 = 3, Q3 = 12.5
         IQR = 12.5 - 3 = 9.5.
         Q1 – 1.5xIQR = 3 – 1.5(9.5)
         = 3 -14.25 = -11.25
         Anything less than -11.25 is an outlier.
         
12. Bagging: Bagging technique can be an effective approach to reduce the variance of a model, to prevent over-fitting and to increase the accuracy of unstable models. 
              1.Aim to decrease variance, not bias.	
              2.Random forest.
    Boosting: Boosting enables us to implement a strong model by combining a number of weak models together.
              1.Aim to decrease bias, not variance. 
              2.Gradient boosting.
              
13. The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the
    new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.   
    To calculate Adjusted R-squared:
      R^2 (adjusted)= 1-(1-R^2)(N-1)    
                        ____________      where R^2 = sample R-squar
                           N-p-1                p = Number of predictors , N = Total sample size.
                           
14. Normalization or Min-Max Scaling is used to transform features to be on a similar scale.                      
     X_new = (X - X_min)/(X_max - X_min) This scales the range to [0, 1] or sometimes [-1, 1].
    Standardization or Z-Score Normalization is the transformation of features by subtracting from mean and dividing by standard deviation. This is often called as Z-score.
     X_new = (X - mean)/Std  Standardization can be helpful in cases where the data follows a Gaussian distribution.
     
15. Cross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the complementary subset of the data-set.
    Advantage : Cross Validation helps in finding the optimal value of hyperparameters to increase the efficiency of the algorithm
    Disadvantage :  Cross Validation drastically increases the training time.           
